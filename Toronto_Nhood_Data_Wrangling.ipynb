{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toronto Neighborhood Data Project\n",
    "\n",
    "## Scope of the Project\n",
    "\n",
    "The idea is to follow a similar neighborhood clustering analysis that we performed for Manhattan (in the NYC_Neighborhood_Segmentation notebook). However, there are a few challenges we need to address.\n",
    "\n",
    "The neighborhood data for New York was provided in an easily accessible json file for us to use. In this case we will be using Beautiful Soup to scrape the Toronto neighborhood data from Wikipedia.  Additionally, since we are scraping web data to get our neighborhoods, there will be a lot more data cleaning and formatting work that we need to do before we can start any sort of analysis.\n",
    "\n",
    "### Assignment #1 - Data Scraping\n",
    "\n",
    "Here we use the Beautiful Soup library to get data from web pages (in particular, Wikipedia pages on Toronto).\n",
    "\n",
    "The Wikipedia page we are using as the basis for our Toronto neighborhoods can be found <a href=\"https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M\">https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "import folium\n",
    "\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a pandas dataframe with the Toronto postal codes as well as the borough and neighborhoods that are associated with each postal code.  \n",
    "\n",
    "There is a bit of data-cleaning that we need to do as well:\n",
    "\n",
    "* First, we remove \"Not Assigned\" postal codes.  These are postal codes that have no neighborhood or borough associated with them.\n",
    "\n",
    "* Next, we make sure that if more than one neighborhood belongs to the same postal code, they are all on the same line and we separate them by a comma. (The current wikipedia table seems to do this already, but it's a good check to perform anyway and in case the wikipedia table changes in the future.)\n",
    "\n",
    "* Then, we make sure that if there is a neighborhood without a name that belongs to a Borough (so the Neighborhood value is \"Not Assigned\" but the Borough name is there), we assign the Borough name to the neighborhorhood value. (Again, the current wikipedia table seems to have taken care of this already, but still worth guarding against future changes.)\n",
    "\n",
    "* Finally, we check the shape of the dataframe to make sure we have the number of rows that we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the requests package to get the postal code page from Wikipedia\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'\n",
    "page = requests.get(url).text\n",
    "\n",
    "# Use beautiful soup to get a cleaned up html file of the page, and to extract the table from the file\n",
    "# This is not strictly necessary, we could probably get away with using pandas read_html file directly on the page,\n",
    "# but this is a bit more robust, and could handle more cleaning of the web source if it were necessary.\n",
    "post_code_page = BeautifulSoup(page, 'html.parser')\n",
    "post_code_table = post_code_page.find('table')\n",
    "\n",
    "# Use the read_html function in pandas to create a dataframe from the table\n",
    "post_code_df = pd.read_html(str(post_code_table))[0]\n",
    "\n",
    "# Drop postal codes where Borough is \"Not assigned\"\n",
    "post_code_df = post_code_df[post_code_df['Borough'] != \"Not assigned\"]\n",
    "\n",
    "# In case there are multiple neighborhoods belonging to the same postal code, we group these together and drop the duplicates\n",
    "post_code_df['Neighbourhood'] = post_code_df.groupby('Postal Code')['Neighbourhood'].transform(lambda x: ', '.join(x))\n",
    "post_code_df = post_code_df.drop_duplicates()\n",
    "\n",
    "# If a neighborhood does not have a name assigned, but does have a Borough assigned, then make the neighbourhood name the same as the Borough\n",
    "post_code_df['Neighbourhood'].replace('Not assigned', post_code_df['Borough'], inplace = True)\n",
    "\n",
    "# Check that the dataframe is as expected\n",
    "post_code_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_code_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment #2 - Adding Geo Coordinates to the Data Frame\n",
    "\n",
    "First, we need to get latitude and longitude for each of the postal codes.  We load a file, **Geospatial_Coordinates.csv** that has this information. Currently the Google API for accessing this data is no longer free. There is a free alternative, geocoder, but it does not seem to provide consistent or reliable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file into a pandas dataframe\n",
    "geo_coordinates = pd.read_csv('Geospatial_Coordinates.csv')\n",
    "\n",
    "# Check the data\n",
    "geo_coordinates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two data frames based on postal code\n",
    "\n",
    "post_code_coordinates = post_code_df.merge(geo_coordinates, how='left', on='Postal Code')\n",
    "\n",
    "post_code_coordinates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment #3 - Exploring and Clustering Neighborhoods in the Toronto Area\n",
    "\n",
    "Finally, we use the foursquare API and folium to explore, cluster and visualize neighborhoods in the Toronto area.\n",
    "\n",
    "First, we use geocoders to get the latitude and longitude of Toronto and then build a map of the area with Folium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coordinates for Toronto, Ontario\n",
    "address = 'Toronto, ON'\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"to_explorer\")\n",
    "location = geolocator.geocode(address)\n",
    "latitude = location.latitude\n",
    "longitude = location.longitude\n",
    "print(\"The geographical coordinates of Toronto are {}, {}.\".format(latitude, longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use folium to create a map of the Toronto\n",
    "\n",
    "map_toronto = folium.Map(location = [latitude, longitude], zoom_start = 10)\n",
    "\n",
    "for lat, lng, borough, neighbourhood in zip(post_code_coordinates['Latitude'], post_code_coordinates['Longitude'], post_code_coordinates['Borough'], post_code_coordinates['Neighbourhood']):\n",
    "    label = '{},{}'.format(neighbourhood, borough)\n",
    "    label = folium.Popup(label,parse_html = True)\n",
    "    folium.CircleMarker(\n",
    "        [lat,lng],\n",
    "        radius = 5,\n",
    "        popup = label,\n",
    "        color = 'blue',\n",
    "        fill = True,\n",
    "        fill_color = '#3186cc',\n",
    "        fill_opacity = 0.7,\n",
    "        parse_html = False).add_to(map_toronto)\n",
    "map_toronto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a new dataframe containing only the neighbourhoods in downtown Toronto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downtown_toronto = post_code_coordinates[post_code_coordinates['Borough'] == 'Downtown Toronto'].reset_index(drop = True)\n",
    "downtown_toronto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downtown_toronto.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coordinates of downtown toronto for a new Folium map\n",
    "\n",
    "address = 'Downtown Toronto, ON'\n",
    "\n",
    "geolocator = Nominatim(user_agent = 'to_explorer')\n",
    "location = geolocator.geocode(address)\n",
    "latitude = location.latitude\n",
    "longitude = location.longitude\n",
    "print('The geographical coordinates of Downtown Toronto are {}, {}.'.format(latitude, longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map of Downtown Toronto\n",
    "\n",
    "map_downtown_toronto = folium.Map(location = [latitude, longitude], zoom_start = 13)\n",
    "\n",
    "for lat, lng, label in zip(downtown_toronto['Latitude'], downtown_toronto['Longitude'], downtown_toronto['Neighbourhood']):\n",
    "    label = folium.Popup(label, parse_html = True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius = 5,\n",
    "        popup = label,\n",
    "        color = 'blue',\n",
    "        fill = True,\n",
    "        fill_color = '#3186cc',\n",
    "        fill_opacity = 0.7,\n",
    "        parse_html = False).add_to(map_downtown_toronto)\n",
    "\n",
    "map_downtown_toronto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we setup access to the foursquare API and get latitude and longitude values for each neighbourhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to foursquare\n",
    "\n",
    "CLIENT_ID = 'P4GV2G4LEKK4XMTHM0H4H5W3CFR055TAHR2IQ3JPLJ0TFHEK'\n",
    "CLIENT_SECRET = 'CJ2YTC5JCO4F3PNHKPSZPMFOTIU5RYD4QD1WB5FYRCS4FD5B'\n",
    "VERSION ='20180604'\n",
    "LIMIT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbourhood_latitude = downtown_toronto.loc[0, 'Latitude']\n",
    "neighbourhood_longitude = downtown_toronto.loc[0, 'Longitude']\n",
    "\n",
    "neighbourhood_name = downtown_toronto.loc[0, 'Neighbourhood']\n",
    "\n",
    "print('Latitude and longitude values of {} are {}, {}'.format(neighbourhood_name, neighbourhood_latitude, neighbourhood_longitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we get the top 100 venues in the Harbourfront and Regent Park neighbourhoods (within a radius of 500 meters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = ''\n",
    "radius = 500\n",
    "\n",
    "url = 'https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&ll={},{}&v={}&query={}&radius={}&limit={}'.format(CLIENT_ID,CLIENT_SECRET,neighbourhood_latitude,neighbourhood_longitude,VERSION,search_query,radius,LIMIT)\n",
    "\n",
    "results = requests.get(url).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that extracts the categories of the venues\n",
    "\n",
    "def get_category_type(row):\n",
    "    try:\n",
    "        categories_list = row['categories']\n",
    "    except:\n",
    "        categories_list = row['venue.categories']\n",
    "    \n",
    "    if len(categories_list) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return categories_list[0]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put this data into a pandas dataframe\n",
    "\n",
    "nearby_venues = results['response']['venues']\n",
    "nearby_venues = pd.json_normalize(nearby_venues)\n",
    "\n",
    "filtered_columns = ['name','categories','location.lat','location.lng']\n",
    "nearby_venues = nearby_venues.loc[:,filtered_columns]\n",
    "\n",
    "nearby_venues['categories'] = nearby_venues.apply(get_category_type, axis = 1)\n",
    "\n",
    "nearby_venues.columns = [col.split(\".\")[-1] for col in nearby_venues.columns]\n",
    "\n",
    "nearby_venues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many venues foursquare returned\n",
    "\n",
    "print('{} venues were returned by Foursquare'.format(nearby_venues.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting Venues for all Downtown Toronto Neighbourhoods\n",
    "\n",
    "Now, we repeat this process for all of the neighborhoods in Downtown Toronto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get nearby venues\n",
    "\n",
    "def getNearbyVenues(names, latitudes, longitudes, radius=500):\n",
    "    \n",
    "    venues_list=[]\n",
    "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
    "        print(name)\n",
    "        \n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(CLIENT_ID,CLIENT_SECRET,VERSION,lat,lng,radius,LIMIT)\n",
    "        results = requests.get(url).json()['response']['groups'][0]['items']\n",
    "        \n",
    "        venues_list.append([(\n",
    "            name,\n",
    "            lat,\n",
    "            lng,\n",
    "            v['venue']['name'],\n",
    "            v['venue']['location']['lat'],\n",
    "            v['venue']['location']['lng'],\n",
    "            v['venue']['categories'][0]['name']) for v in results])\n",
    "\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_venues.columns = ['Neighbourhood', 'Neighbourhood Latitude','Neighbourhood Longitude','Venue','Venue Latitude','Venue Longitude','Venue Category']\n",
    "    \n",
    "    return(nearby_venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downtown_venues = getNearbyVenues(names = downtown_toronto['Neighbourhood'], latitudes = downtown_toronto['Latitude'], longitudes = downtown_toronto['Longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(downtown_venues.shape)\n",
    "downtown_venues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many unique categories from all the returned venues\n",
    "\n",
    "print('There are {} unique categories.'.format(len(downtown_venues['Venue Category'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neighbourhood analysis\n",
    "\n",
    "downtown_onehot = pd.get_dummies(downtown_venues[['Venue Category']], prefix = '', prefix_sep='')\n",
    "\n",
    "downtown_onehot['Neighbourhood'] = downtown_venues['Neighbourhood']\n",
    "\n",
    "fixed_columns = [downtown_onehot.columns[-1]] + list(downtown_onehot.columns[:-1])\n",
    "downtown_onehot = downtown_onehot[fixed_columns]\n",
    "\n",
    "downtown_onehot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downtown_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downtown_grouped = downtown_onehot.groupby('Neighbourhood').mean().reset_index()\n",
    "downtown_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we display the top 5 most common venues for each neighbourhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_venues = 5\n",
    "\n",
    "for hood in downtown_grouped['Neighbourhood']:\n",
    "    print(\"----\"+hood+\"----\")\n",
    "    temp = downtown_grouped[downtown_grouped['Neighbourhood'] == hood].T.reset_index()\n",
    "    temp.columns = ['venue','freq']\n",
    "    temp = temp.iloc[1:]\n",
    "    temp['freq'] = temp['freq'].astype(float)\n",
    "    temp = temp.round({'freq': 2})\n",
    "    print(temp.sort_values('freq', ascending = False).reset_index(drop = True).head(num_top_venues))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Analysis of Neighbourhood Venues\n",
    "\n",
    "First, we'll get this data into a pandas dataframe.\n",
    "\n",
    "Then, we do some sorting on the most common venues, and display the top venues for each neighbouhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sort venues\n",
    "\n",
    "def return_most_common_venues(row, num_top_venues):\n",
    "    row_categories = row.iloc[1:]\n",
    "    row_categories_sorted = row_categories.sort_values(ascending = False)\n",
    "    \n",
    "    return row_categories_sorted.index.values[0:num_top_venues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New data frame to display the top 10 venues for each neighbourhood\n",
    "\n",
    "num_top_venues = 10\n",
    "\n",
    "indicators = ['st', 'nd', 'rd']\n",
    "\n",
    "columns = ['Neighbourhood']\n",
    "for ind in np.arange(num_top_venues):\n",
    "    try:\n",
    "        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n",
    "    except:\n",
    "        columns.append('{}th Most Common Venue'.format(ind+1))\n",
    "    \n",
    "\n",
    "neighbourhoods_venues_sorted = pd.DataFrame(columns = columns)\n",
    "neighbourhoods_venues_sorted['Neighbourhood'] = downtown_grouped['Neighbourhood']\n",
    "\n",
    "for ind in np.arange(downtown_grouped.shape[0]):\n",
    "    neighbourhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(downtown_grouped.iloc[ind, :], num_top_venues)\n",
    "\n",
    "neighbourhoods_venues_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neighbourhood Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of clusters\n",
    "\n",
    "kclusters = 3\n",
    "\n",
    "downtown_grouped_clustering = downtown_grouped.drop('Neighbourhood', 1)\n",
    "\n",
    "kmeans = KMeans(n_clusters = kclusters, random_state = 0).fit(downtown_grouped_clustering)\n",
    "\n",
    "kmeans.labels_[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe that includes cluster and top 10 venues for each neighbourhood\n",
    "\n",
    "neighbourhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)\n",
    "\n",
    "downtown_merged = downtown_toronto\n",
    "\n",
    "downtown_merged = downtown_merged.join(neighbourhoods_venues_sorted.set_index('Neighbourhood'), on = 'Neighbourhood')\n",
    "\n",
    "downtown_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_clusters = folium.Map(location=[latitude,longitude], zoom_start = 13)\n",
    "\n",
    "x = np.arange(kclusters)\n",
    "ys = [i + x + (i*x)**2 for i in range(kclusters)]\n",
    "colors_array = cm.rainbow(np.linspace(0,1,len(ys)))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "markers_colors = []\n",
    "for lat, lon, poi, cluster in zip(downtown_merged['Latitude'], downtown_merged['Longitude'], downtown_merged['Neighbourhood'], downtown_merged['Cluster Labels']):\n",
    "    label = folium.Popup(str(poi) + ' Cluster' + str(cluster), parse_html = True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lon],\n",
    "        radius = 5,\n",
    "        popup = label,\n",
    "        color = rainbow[cluster -1],\n",
    "        fill = True,\n",
    "        fill_color = rainbow[cluster -1],\n",
    "        fill_opacity = 0.7).add_to(map_clusters)\n",
    "\n",
    "map_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Each Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downtown_merged.loc[downtown_merged['Cluster Labels'] == 0, downtown_merged.columns[[1] + list(range(5, downtown_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downtown_merged.loc[downtown_merged['Cluster Labels'] == 1, downtown_merged.columns[[1] + list(range(5, downtown_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downtown_merged.loc[downtown_merged['Cluster Labels'] == 2, downtown_merged.columns[[1] + list(range(5, downtown_merged.shape[1]))]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
