{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera Capstone Project Using Seattle Collision Data\n",
    "\n",
    "### This is a Jupyter notebook we will be using to analyze and present findings based on collision data in the city of Seattle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and Business Understanding\n",
    "\n",
    "## Overview\n",
    "\n",
    "There are more than 10,000 traffic collisions per year involving cars, bicyclists and pedestrians. Understanding the causes of collisions as well as the conditions that impact their severity will help provide insight to officials on how to better allocate resources to help reduce the number and severity of such incidents.\n",
    "\n",
    "Further, a better understanding of the factors that increase the likelihood of collisions and increase the probability of injury or property damage can help with education efforts to help individuals take greater precautions when making travel decisions.\n",
    "\n",
    "## Goals of the Project\n",
    "\n",
    "The goal of the project is to use publicly available data compiled by the Seattle Deport of Transportation (SDOT), to identify feautures in the dataset that yield predictive information on the number and severity of collisions and injuries in Seattle.\n",
    "\n",
    "We will also look to use data visualization tools to communicate this information and provide an overview of the current state of traffic collisions in Seattle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "\n",
    "The dataset we are using is *Collisions - All Years* dataset maintained by the SDOT Traffic Management Division's Traffic Records Group.  This dataset includes all types of collisions, including car, bicycle, and pedestrian as provided by the Seattle Police Department in their Traffic Records.\n",
    "\n",
    "The data set contains information on over 194,000 collisions in Seattle over a 15-year period.  The primary attribute we are looking to predict is the severity of the collision as captured by the Severity Code assigned to the collision.  Interestingly, the dataset description provided by SDOT indicates this Severity Code attribute should take values between 0 and 3 (including both 2 and 2b to differential \"injury\" from \"serious injury\"); however, the actual data set only contains the values 1 and 2 for this attribute.  One avenue to explore in a future project is to find additional information on this target attribute.\n",
    "\n",
    "The data includes 37 different features including: day, time, month, lighting conditions, road conditions and weather conditions.\n",
    "\n",
    "A full description of the data can be found at: https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/DP0701EN/version-2/Metadata.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing some packages we'll want.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pylab as pl\n",
    "import scipy.optimize as opt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"~/Documents/CertificationStuff/IBMPythonDataScience/Data_Science_Capstone/Data-Collisions.csv\"\n",
    "\n",
    "df = pd.read_csv(path, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows without latitude and longitude\n",
    "df.dropna(subset=[\"X\"],axis=0,inplace=True)\n",
    "\n",
    "# Replace the rows that are missing Coordinates with the average coordinate \n",
    "#mean_longtitude = df[\"X\"].mean()\n",
    "#mean_latitude = df[\"Y\"].mean()\n",
    "#df[\"X\"].replace(np.nan, mean_longitude, inplace = True)\n",
    "#df[\"Y\"].replace(np.nan, mean_latitutde, inplace = True)\n",
    "\n",
    "# Drop Unnecessary or Redundant Columns\n",
    "df.drop(['SEVERITYCODE.1','OBJECTID','INCKEY','COLDETKEY','REPORTNO','STATUS','ADDRTYPE','INTKEY','LOCATION','EXCEPTRSNCODE','EXCEPTRSNDESC','SDOT_COLCODE','SDOT_COLDESC','PEDROWNOTGRNT','SDOTCOLNUM','ST_COLCODE','ST_COLDESC','SEGLANEKEY','CROSSWALKKEY'],axis = 1, inplace = True)\n",
    "\n",
    "df['SEVERITYCODE'] = df['SEVERITYCODE'] - 1\n",
    "\n",
    "# Replace string values with Boolean values in some appropriate Columns\n",
    "df['INATTENTIONIND'].replace(np.nan, 0, inplace = True)\n",
    "df['INATTENTIONIND'].replace(to_replace='Y', value = 1, inplace = True)\n",
    "\n",
    "df['UNDERINFL'].replace(np.nan, 0, inplace = True)\n",
    "df['UNDERINFL'].replace('N', 0, inplace = True)\n",
    "df['UNDERINFL'].replace('*',0, inplace= True)\n",
    "df['UNDERINFL'].replace('Y',1,inplace = True)\n",
    "df['UNDERINFL'] = df['UNDERINFL'].astype(int)\n",
    "\n",
    "df['COLLISIONTYPE'].replace(np.nan, \"Other\", inplace = True)\n",
    "\n",
    "df['SPEEDING'].replace(np.nan, False, inplace = True)\n",
    "df['SPEEDING'].replace('Y', True, inplace = True)\n",
    "df['SPEEDING'] = df['SPEEDING'].astype(int)\n",
    "\n",
    "df['HITPARKEDCAR'].replace('N', False, inplace = True)\n",
    "df['HITPARKEDCAR'].replace('Y', True, inplace = True)\n",
    "df['HITPARKEDCAR'] = df['HITPARKEDCAR'].astype(int)\n",
    "\n",
    "# Consolidate missing, \"Unknown\", NaN values in some columns to \"Unknown\"\n",
    "df['WEATHER'].replace(np.nan,\"Unknown\", inplace = True)\n",
    "df['WEATHER'].replace(\"Other\", \"Unknown\", inplace = True)\n",
    "\n",
    "df['ROADCOND'].replace(np.nan, \"Unknown\", inplace = True)\n",
    "df['ROADCOND'].replace(\"Other\", \"Unknown\", inplace = True)\n",
    "\n",
    "df['LIGHTCOND'].replace(np.nan, \"Unknown\", inplace = True)\n",
    "df['LIGHTCOND'].replace(\"Other\", \"Unknown\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_severities_counts = df['SEVERITYDESC'].value_counts().to_frame()\n",
    "print(possible_severities_counts)\n",
    "print()\n",
    "\n",
    "possible_weather_conditions = df['WEATHER'].value_counts().to_frame()\n",
    "print(possible_weather_conditions)\n",
    "print()\n",
    "\n",
    "possible_road_conditions = df['ROADCOND'].value_counts().to_frame()\n",
    "print(possible_road_conditions)\n",
    "print()\n",
    "\n",
    "possible_lighting_conditions = df['LIGHTCOND'].value_counts().to_frame()\n",
    "print(possible_lighting_conditions)\n",
    "print()\n",
    "\n",
    "speeding = df['SPEEDING'].value_counts().to_frame()\n",
    "print(speeding)\n",
    "print()\n",
    "\n",
    "under_influence = df['UNDERINFL'].value_counts().to_frame()\n",
    "print(under_influence)\n",
    "print()\n",
    "\n",
    "collision_type = df['COLLISIONTYPE'].value_counts().to_frame()\n",
    "print(collision_type)\n",
    "\n",
    "# print(\"Possible Severities: \", df['SEVERITYDESC'].unique()) \n",
    "# print(\"Weather Conditions: \", df['WEATHER'].unique())\n",
    "# print(\"Road Conditions: \", df['ROADCOND'].unique())\n",
    "# print(\"Lighting Conditions: \", df['LIGHTCOND'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put some types of accidents into data frames so we can analyze\n",
    "\n",
    "df_icy_road = df[df['ROADCOND']=='Ice']\n",
    "df_wet_road = df[df['ROADCOND']=='Wet']\n",
    "df_snow_road = df[df['ROADCOND']=='Snow/Slush']\n",
    "\n",
    "df_speeding = df[df['SPEEDING'] == 1]\n",
    "df_under_influence = df[df['UNDERINFL'] == 1]\n",
    "\n",
    "df_dark = df[df['LIGHTCOND'] == 'Dark - Street Lights Off']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latitude and longitude of center of Seattle\n",
    "latitude = 47.6062\n",
    "longitude = -122.3321\n",
    "\n",
    "# Create map Seattle\n",
    "seattle_map = folium.Map(location=[latitude,longitude],zoom_start=11)\n",
    "\n",
    "# Display map of Seattle\n",
    "seattle_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding features to the map\n",
    "\n",
    "dark_collision = folium.map.FeatureGroup()\n",
    "\n",
    "for lat,lng in zip(df_dark.Y, df_dark.X):\n",
    "    dark_collision.add_child(\n",
    "        folium.features.CircleMarker(\n",
    "            [lat,lng],\n",
    "            radius = 5,\n",
    "            color = 'yellow',\n",
    "            fill = True,\n",
    "            fill_color = 'blue',\n",
    "            fill_opacity = 0.6\n",
    "        )\n",
    "    )\n",
    "\n",
    "# snow_collisions = folium.map.FeatureGroup()\n",
    "\n",
    "# for lat,lng in zip(df_snow_road.Y, df_snow_road.X):\n",
    "#    snow_collisions.add_child(\n",
    "#        folium.features.CircleMarker(\n",
    "#            [lat,lng],\n",
    "#            radius = 5,\n",
    "#            color='yellow',\n",
    "#            fill = True,\n",
    "#            fill_color='blue',\n",
    "#            fill_opacity = 0.6\n",
    "#        )\n",
    "#    )\n",
    "\n",
    "seattle_map.add_child(dark_collision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium import plugins\n",
    "\n",
    "seattle_map = folium.Map(location=[latitude,longitude],zoom_start=11)\n",
    "\n",
    "dark_collision = plugins.MarkerCluster().add_to(seattle_map)\n",
    "\n",
    "for lat, lng, label in zip (df_dark.Y, df_dark.X, df_dark.SEVERITYDESC):\n",
    "    folium.Marker(\n",
    "        location = [lat,lng],\n",
    "        icon = None,\n",
    "        popup = label,\n",
    "    ).add_to(dark_collision)\n",
    "    \n",
    "seattle_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "All the code to create a visualize a decision tree for predicting accident severity.  It didn't seem to yield much results.\n",
    "\n",
    "\n",
    "!conda install -c conda-forge pydotplus -y\n",
    "!conda install -c conda-forge python-graphviz -y\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.externals.six import StringIO\n",
    "import pydotplus\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn import tree\n",
    "%matplotlib inline\n",
    "\n",
    "df_decision = df[['SEVERITYDESC','WEATHER','ROADCOND','LIGHTCOND','SPEEDING','UNDERINFL']]\n",
    "df_decision = df_decision[df_decision.WEATHER != 'Unknown']\n",
    "df_decision = df_decision[df_decision.ROADCOND != 'Unknown']\n",
    "df_decision = df_decision[df_decision.LIGHTCOND != 'Unknown']\n",
    "\n",
    "\n",
    "X = df_decision[['WEATHER','ROADCOND','LIGHTCOND','SPEEDING','UNDERINFL']].values\n",
    "\n",
    "le_weather = preprocessing.LabelEncoder()\n",
    "le_weather.fit(['Blowing Sand/Dirt', 'Clear', 'Fog/Smog/Smoke', 'Overcast', 'Partly Cloudy', 'Raining', 'Severe Crosswind', 'Sleet/Hail/Freezing Rain', 'Snowing'])\n",
    "X[:,0] = le_weather.transform(X[:,0])\n",
    "\n",
    "le_road = preprocessing.LabelEncoder()\n",
    "le_road.fit(['Dry','Wet','Ice','Snow/Slush','Standing Water','Sand/Mud/Dirt','Oil'])\n",
    "X[:,1] = le_road.transform(X[:,1])\n",
    "\n",
    "le_light = preprocessing.LabelEncoder()\n",
    "le_light.fit(['Daylight','Dark - Street Lights On','Dusk','Dawn','Dark - No Street Lights','Dark - Street Lights Off','Dark - Unknown Lighting'])\n",
    "X[:,2] = le_light.transform(X[:,2])\n",
    "\n",
    "X[0:5]\n",
    "\n",
    "y = df_decision['SEVERITYDESC']\n",
    "y[0:5]\n",
    "\n",
    "X_trainset, X_testset, y_trainset, y_testset = train_test_split(X,y,test_size = 0.3, random_state = 3)\n",
    "\n",
    "severityTree = DecisionTreeClassifier(criterion = \"entropy\", max_depth = 4)\n",
    "severityTree\n",
    "\n",
    "severityTree.fit(X_trainset, y_trainset)\n",
    "\n",
    "predictionTree = severityTree.predict(X_testset)\n",
    "\n",
    "print (predictionTree [0:5])\n",
    "print (y_testset [0:5])\n",
    "\n",
    "print (\"Decision Tree Accuracy: \", metrics.accuracy_score(y_testset, predictionTree))\n",
    "\n",
    "dot_data = StringIO()\n",
    "filename = \"severityTree.png\"\n",
    "featureNames = ['WEATHER','ROADCOND','LIGHTCOND','SPEEDING','UNDERINFL']\n",
    "targetNames = df[\"SEVERITYDESC\"].unique().tolist()\n",
    "out = tree.export_graphviz(severityTree,feature_names = featureNames, out_file = dot_data, class_names = np.unique(y_trainset), filled = True, special_characters = True, rotate = False)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "graph.write_png(filename)\n",
    "img = mpimg.imread(filename)\n",
    "plt.figure(figsize = (100,200))\n",
    "plt.imshow(img, interpolation = 'nearest')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "import scipy.optimize as opt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_df = df[['SEVERITYCODE','COLLISIONTYPE','UNDERINFL','WEATHER','ROADCOND','LIGHTCOND','SPEEDING']]\n",
    "\n",
    "severity_df = severity_df[severity_df.WEATHER != 'Unknown']\n",
    "severity_df = severity_df[severity_df.ROADCOND != 'Unknown']\n",
    "severity_df = severity_df[severity_df.LIGHTCOND != 'Unknown']\n",
    "\n",
    "X = np.asarray(severity_df[['COLLISIONTYPE','UNDERINFL','WEATHER','ROADCOND','LIGHTCOND','SPEEDING']])\n",
    "\n",
    "le_collision_type = preprocessing.LabelEncoder()\n",
    "le_collision_type.fit(['Parked Car','Angles','Rear Ended','Other','Sideswipe','Left Turn','Pedestrian','Cycles','Right Turn','Head On'])\n",
    "X[:,0] = le_collision_type.transform(X[:,0])\n",
    "\n",
    "le_weather = preprocessing.LabelEncoder()\n",
    "le_weather.fit(['Blowing Sand/Dirt', 'Clear', 'Fog/Smog/Smoke', 'Overcast', 'Partly Cloudy', 'Raining', 'Severe Crosswind', 'Sleet/Hail/Freezing Rain', 'Snowing'])\n",
    "X[:,2] = le_weather.transform(X[:,2])\n",
    "\n",
    "le_road = preprocessing.LabelEncoder()\n",
    "le_road.fit(['Dry','Wet','Ice','Snow/Slush','Standing Water','Sand/Mud/Dirt','Oil'])\n",
    "X[:,3] = le_road.transform(X[:,3])\n",
    "\n",
    "le_light = preprocessing.LabelEncoder()\n",
    "le_light.fit(['Daylight','Dark - Street Lights On','Dusk','Dawn','Dark - No Street Lights','Dark - Street Lights Off','Dark - Unknown Lighting'])\n",
    "X[:,4] = le_light.transform(X[:,4])\n",
    "\n",
    "X[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.asarray(severity_df['SEVERITYCODE'])\n",
    "y[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessing.StandardScaler().fit(X).transform(X)\n",
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 4)\n",
    "print('Train set:', X_train.shape, y_train.shape)\n",
    "print('Test set:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(C=0.01,solver='liblinear').fit(X_train,y_train)\n",
    "LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = LR.predict(X_test)\n",
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_prob = LR.predict_proba(X_test)\n",
    "yhat_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_similarity_score\n",
    "jaccard_similarity_score(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes, normalize = False, title = 'Confusion matrix', cmap = plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    \n",
    "    print(cm)\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks=np.arange(len(classes))\n",
    "    plt.xticks(tick_marks,classes,rotation=45)\n",
    "    plt.yticks(tick_marks,classes)\n",
    "    \n",
    "    fmt='.2f' if normalize else 'd'\n",
    "    thresh=cm.max() / 2.\n",
    "    for i,j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):\n",
    "        plt.text(j,i,format(cm[i,j],fmt),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i,j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "print(confusion_matrix(y_test,yhat,labels=[1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_test,yhat,labels=[1,0])\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix,classes=['Injury = 1', 'Injury = 0'],normalize = False, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (classification_report(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_test,yhat_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START SVM Code\n",
    "\n",
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='rbf')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = clf.predict(X_test)\n",
    "yhat[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize = False, title = 'Confusion matrix', cmap = plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    \n",
    "    print(cm)\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks=np.arange(len(classes))\n",
    "    plt.xticks(tick_marks,classes,rotation=45)\n",
    "    plt.yticks(tick_marks,classes)\n",
    "    \n",
    "    fmt='.2f' if normalize else 'd'\n",
    "    thresh=cm.max() / 2.\n",
    "    for i,j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):\n",
    "        plt.text(j,i,format(cm[i,j],fmt),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i,j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "print(confusion_matrix(y_test,yhat,labels=[1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_test, yhat, labels=[1,0])\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(classification_report(y_test,yhat))\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes = ['No Injury', 'Injury'], normalize = False, title='Confusion Matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
